{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis methods\n",
    "\n",
    "There is no ML method which solves specifically the problem of sentiment analysis, so it is important to test some different algorithms and choose the best one. In our tests we are going to use scikit-learn library, which is commonly used, especially for implementing POCs.\n",
    "\n",
    "The quality of the ML model has to be measured somehow in order to compare their efficiency for given problem. In our case we are going to use quite a simple metric, called accuracy. \n",
    "\n",
    "$$accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
    "\n",
    "We have three different labels for our texts: positive, negative and neutral. To avoid dealing with texts, we are going to convert them to 1, -1 and 0, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 02_data_preparation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with defining a base pipeline for our dataset. We're going to use some functions declared in previous parts of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SENTIMENT_TO_LABEL_MAPPING = {\n",
    "    \"negative\": -1,\n",
    "    \"neutral\": 0,\n",
    "    \"positive\": 1\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "raw_tweets = pd.read_csv(\"data/twitter-airlines-sentiment.csv\")\n",
    "\n",
    "# Preprocess the data with the function declared previously\n",
    "tweets = raw_tweets[[\"airline_sentiment\", \"text\"]]\n",
    "tweets.columns = (\"sentiment\", \"text\", )\n",
    "tweets[\"text\"] = tweets[\"text\"].map(preprocess_text)\n",
    "tweets[\"sentiment\"] = tweets[\"sentiment\"].map(lambda x: SENTIMENT_TO_LABEL_MAPPING[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Divide the dataset into train and test fraction\n",
    "train_messages, test_messages, train_targets, test_targets = train_test_split(tweets[\"text\"], \n",
    "                                                                              tweets[\"sentiment\"],\n",
    "                                                                              test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 s, sys: 1.65 s, total: 2.76 s\n",
      "Wall time: 1.54 s\n",
      "CPU times: user 211 ms, sys: 180 ms, total: 391 ms\n",
      "Wall time: 396 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy score:0.6164617486338798\n",
      "\n",
      "CPU times: user 14 s, sys: 612 ms, total: 14.6 s\n",
      "Wall time: 14.6 s\n",
      "CPU times: user 18min 9s, sys: 471 ms, total: 18min 10s\n",
      "Wall time: 18min 12s\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.5283469945355191\n",
      "\n",
      "CPU times: user 19.1 s, sys: 800 ms, total: 19.9 s\n",
      "Wall time: 19.8 s\n",
      "CPU times: user 18min 12s, sys: 368 ms, total: 18min 12s\n",
      "Wall time: 18min 13s\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.476775956284153\n",
      "\n",
      "CPU times: user 16.2 s, sys: 716 ms, total: 16.9 s\n",
      "Wall time: 16.9 s\n",
      "CPU times: user 18min 13s, sys: 336 ms, total: 18min 13s\n",
      "Wall time: 18min 14s\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=27, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.40027322404371585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Declare vectorizers to be used\n",
    "VECTORIZERS = (\n",
    "    CountVectorizer(),\n",
    "    TfidfVectorizer(),\n",
    ")\n",
    "\n",
    "# Declare classifiers to be used\n",
    "CLASSIFIERS = (\n",
    "    LogisticRegression(C=10e-5, solver=\"liblinear\", max_iter=10000),\n",
    "    KNeighborsClassifier(3),\n",
    "    KNeighborsClassifier(9),\n",
    "    KNeighborsClassifier(27),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    LinearSVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(random_state=2018),\n",
    "    GaussianNB(),\n",
    ")\n",
    "\n",
    "for vectorizer, classifier in itertools.product(VECTORIZERS, CLASSIFIERS):\n",
    "    # Vectorize preprocessed sentences\n",
    "    train_features = vectorizer.fit_transform(train_messages)\n",
    "\n",
    "    # Train the model\n",
    "    %time fit = classifier.fit(train_features.toarray(), train_targets)\n",
    "\n",
    "    # Check the accuracy of the model on test data and display it\n",
    "    test_features = vectorizer.transform(test_messages)\n",
    "    %time test_predictions = fit.predict(test_features.toarray())\n",
    "    accuracy = accuracy_score(test_predictions, test_targets)\n",
    "    print(\"Vectorizer: {}\\nClassifier: {}\\nAccuracy score:{}\\n\".format(vectorizer,\n",
    "                                                                       classifier, \n",
    "                                                                       accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
